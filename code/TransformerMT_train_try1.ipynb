{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "NegqV_ruA7vd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A62MSUuAxAT",
        "outputId": "c1dfc1cf-15af-4831-b47f-b492e2ad6659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install indic-nlp-library --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CusYFV58A6r_",
        "outputId": "06fa100d-f718-4070-901b-e15b5dd55c8a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import spacy\n",
        "from tqdm.notebook import tqdm\n",
        "import sys\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "from indicnlp.normalize import indic_normalize\n",
        "import re\n",
        "import pickle\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "gef_zAybA6p6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data"
      ],
      "metadata": {
        "id": "UdlWjrfBA_ZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/CS779_MT_google_colab/out_sent.pkl', 'rb') as f:\n",
        "  input_sent_all = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/CS779_MT_google_colab/inp_sent.pkl', 'rb') as f:\n",
        "  output_sent_all = pickle.load(f)\n",
        "\n",
        "print(len(input_sent_all))\n",
        "print(input_sent_all[0])\n",
        "\n",
        "print(len(output_sent_all))\n",
        "print(output_sent_all[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkC2SK8yA6mA",
        "outputId": "f0c734b1-b6cd-4d4b-9ccd-9ef0aa2398ae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140000\n",
            "और अपनी रहमत से हमें इन काफ़िर लोगों (के नीचे) से नजात दे\n",
            "140000\n",
            "and deliver us by Thy mercy from the people of the unbelievers. '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Device"
      ],
      "metadata": {
        "id": "YXZmyKZEJ5hP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "print(f\"device = {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mbjmQXrJ78c",
        "outputId": "093b9ee6-9afa-42f0-8668-e5f9ca880f07"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device = cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the vocab"
      ],
      "metadata": {
        "id": "jsuwSTTGBEvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Lang():\n",
        "\n",
        "  def __init__(self, name, spacy_tokenizer):\n",
        "    self.name = name\n",
        "    self.word2index = {\"<SOS>\":0, '<EOS>': 1, \"<PAD>\": 2, '<UNK>': 3}\n",
        "    self.index2word = {0: \"<SOS>\", 1: \"<EOS>\", 2: \"<PAD>\", 3: '<UNK>'}\n",
        "    self.word2count = {}\n",
        "    self.n_words = 4\n",
        "    self.tokenizer = spacy_tokenizer\n",
        "\n",
        "  def add_word(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "\n",
        "    else:\n",
        "      self.word2count[word] += 1\n",
        "\n",
        "  def add_sentence(self, sentence):\n",
        "    tokens = self.tokenize_sentence(sentence)\n",
        "    for token in tokens: \n",
        "      self.add_word(token)\n",
        "\n",
        "  def tokenize_sentence(self, sentence):\n",
        "    tokens = [token.text for token in self.tokenizer(sentence.lower())]\n",
        "    return tokens\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_words\n"
      ],
      "metadata": {
        "id": "y2AMLCkXA6j8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Hindi_lang():\n",
        "\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "    self.word2index = {\"<SOS>\":0, '<EOS>': 1, \"<PAD>\": 2, '<UNK>': 3}\n",
        "    self.index2word = {0: \"<SOS>\", 1: \"<EOS>\", 2: \"<PAD>\", 3: '<UNK>'}\n",
        "    self.word2count = {}\n",
        "    self.n_words = 4\n",
        "    self.normalizer = indic_normalize.DevanagariNormalizer(lang='hi', remove_nuktas=True)\n",
        "\n",
        "  def add_word(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "\n",
        "    else:\n",
        "      self.word2count[word] += 1\n",
        "\n",
        "  def add_sentence(self, sentence):\n",
        "    tokens = self.tokenize_sentence(sentence)\n",
        "    for token in tokens: \n",
        "      self.add_word(token)\n",
        "\n",
        "  def tokenize_sentence(self, sentence):\n",
        "    # first normalize the sentence, then tokenize\n",
        "    norm_sent = self.normalizer.normalize(sentence)\n",
        "    tokens = indic_tokenize.trivial_tokenize(norm_sent)\n",
        "    return tokens\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_words"
      ],
      "metadata": {
        "id": "0o9gYmWwA6io"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/CS779_MT_google_colab/english_output_vocab.pkl', 'rb') as f: \n",
        "  english_output_vocab = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/CS779_MT_google_colab/hindi_input_vocab.pkl', 'rb') as f: \n",
        "  hindi_input_vocab = pickle.load(f)"
      ],
      "metadata": {
        "id": "JlXSZLlSA6gh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_TOKEN_INDEX = english_output_vocab.word2index['<SOS>']\n",
        "PAD_TOKEN_INDEX = english_output_vocab.word2index['<PAD>']\n",
        "EOS_TOKEN_INDEX = english_output_vocab.word2index['<EOS>']"
      ],
      "metadata": {
        "id": "6lrNYUdlA6ec"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model class and some functions"
      ],
      "metadata": {
        "id": "i0HDJLifBqg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self, input_vocab_size, output_vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout):\n",
        "\n",
        "    super().__init__()\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.nhead = nhead \n",
        "    self.num_encoder_layers = num_encoder_layers\n",
        "    self.num_decoder_layers = num_decoder_layers\n",
        "    self.dim_feedforward = dim_feedforward\n",
        "    self.dropout = dropout\n",
        "\n",
        "    self.input_embedding = nn.Embedding(input_vocab_size, d_model)\n",
        "    transformer_enc_layer = nn.TransformerEncoderLayer(self.d_model, self.nhead, self.dim_feedforward, self.dropout, batch_first=True)\n",
        "    self.encoder = nn.TransformerEncoder(transformer_enc_layer, self.num_encoder_layers)\n",
        "\n",
        "    self.output_embedding = nn.Embedding(output_vocab_size, d_model)\n",
        "    transformer_dec_layer = nn.TransformerDecoderLayer(self.d_model, self.nhead, self.dim_feedforward, self.dropout, batch_first=True)\n",
        "    self.decoder = nn.TransformerDecoder(transformer_dec_layer, self.num_decoder_layers)\n",
        "\n",
        "    self.out = nn.Linear(d_model, output_vocab_size)\n",
        "\n",
        "  def forward(self, src, trg):\n",
        "\n",
        "    # src.shape --> (batch_size, seq_len)\n",
        "    # trg.shape --> (batch_size, seq_len)\n",
        "\n",
        "    # output is a tensor of shape : (batch_size, trg_seq_len, output_vocab_size)\n",
        "\n",
        "    src_mask = self.generate_src_mask(src.size(1), trg.size(1)).to(device) # (trg_seq_len, src_seq_len)\n",
        "    trg_mask = self.generate_square_subsequent_mask(trg.size(1)).to(device) # (tgt_seq_len, tgt_seq_len)\n",
        "\n",
        "    src_embed = self.input_embedding(src) # (batch_size, seq_len, embed_dim)\n",
        "    src_enc = self.encoder(src_embed) # (batch_size, seq_len, embed_dim)\n",
        "\n",
        "    src_padding_mask = self.generate_padding_mask(src).to(device) # (batch_size, seq_len)\n",
        "    trg_padding_mask = self.generate_padding_mask(trg).to(device) # (batch_size, seq_len)\n",
        "\n",
        "    trg_embed = self.output_embedding(trg) # (batch_size seq_len, embed_dim)\n",
        "    trg_dec = self.decoder.forward(tgt=trg_embed, memory=src_embed, tgt_mask=trg_mask, memory_mask=src_mask, tgt_key_padding_mask=trg_padding_mask, memory_key_padding_mask=src_padding_mask)\n",
        "\n",
        "    # trg_dec.shape --> (batch_size, tgt_seq_len, embed_dim)\n",
        "\n",
        "    output = self.out(trg_dec) # (batch_size, tgt_seq_len, output_vocab_size)\n",
        "    return output\n",
        "\n",
        "  def generate_src_mask(self, src_length, trg_length):\n",
        "\n",
        "    mask = torch.zeros(trg_length, src_length)\n",
        "    for i in range(trg_length):\n",
        "      mask[i, :i+1] = 1\n",
        "\n",
        "    mask = mask.masked_fill(mask==0, float('-inf')).masked_fill(mask==1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "  def generate_square_subsequent_mask(self, sz):\n",
        "\n",
        "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask==1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "  def generate_padding_mask(self, my_tensor):\n",
        "\n",
        "    # my_tensor.shape --> (batch_size, seq_len)\n",
        "    # returns the padding mask tensor of shape (batch_size, seq_len)\n",
        "\n",
        "    PADDING_INDEX = hindi_input_vocab.word2index['<PAD>']\n",
        "\n",
        "    padding_lists = []\n",
        "    for my_sent in my_tensor:\n",
        "      pad_list = []\n",
        "      for word in my_sent:\n",
        "        if word == PADDING_INDEX:\n",
        "          pad_list.append(0)\n",
        "        else:\n",
        "          pad_list.append(1)\n",
        "\n",
        "      padding_lists.append(pad_list)\n",
        "\n",
        "    padding_tensor = torch.tensor(padding_lists, dtype=torch.float32)\n",
        "    return padding_tensor"
      ],
      "metadata": {
        "id": "394EryA5A6ai"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_batch(model, optimizer, criterion, input_tensors, target_tensors):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  input_tensors = input_tensors.to(device) # shape : (batch_size, seq_len)\n",
        "  target_tensors = target_tensors.to(device) # shape : (batch_size, seq_len)\n",
        "\n",
        "  outputs = model(input_tensors, target_tensors[:, :-1]) \n",
        "  # outputs.shape --> (batch_size, seq_len-1, output_vocab_size)\n",
        "  \n",
        "  outputs = outputs.view(-1, outputs.size(-1))\n",
        "  # shape : (batch_size * (seq_len-1), output_vocab_size)\n",
        "\n",
        "  target_tensors = target_tensors[:, 1:].contiguous().view(-1)\n",
        "\n",
        "  loss = criterion(outputs, target_tensors)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "  # clipping the grad to avoid exploding gradients problem\n",
        "\n",
        "  optimizer.step()\n",
        "  # update the params\n",
        "\n",
        "  return loss.item()"
      ],
      "metadata": {
        "id": "94E3swKvA6YO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the data for the model"
      ],
      "metadata": {
        "id": "WFVSFTbvB5-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "MAX_LENGTH = 64"
      ],
      "metadata": {
        "id": "U3iiAplsGCtd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_seqs = []\n",
        "output_seqs = []\n",
        "\n",
        "for input_sent, output_sent in zip(input_sent_all, output_sent_all):\n",
        "  input_seq = [hindi_input_vocab.word2index[word] for word in hindi_input_vocab.tokenize_sentence(input_sent)]\n",
        "  output_seq = [english_output_vocab.word2index[word] for word in english_output_vocab.tokenize_sentence(output_sent)]\n",
        "\n",
        "  if len(input_seq) < MAX_LENGTH:\n",
        "    input_seq += [hindi_input_vocab.word2index['<PAD>']] * (MAX_LENGTH - len(input_seq))\n",
        "  else:\n",
        "    input_seq = input_seq[:64]\n",
        "\n",
        "  if len(output_seq) < MAX_LENGTH:\n",
        "    output_seq += [english_output_vocab.word2index['<PAD>']] * (MAX_LENGTH - len(output_seq))\n",
        "  else:\n",
        "    output_seq = output_seq[:MAX_LENGTH]\n",
        "\n",
        "  input_seqs.append(torch.tensor(input_seq, dtype=torch.long))\n",
        "  output_seqs.append(torch.tensor(output_seq, dtype=torch.long))\n",
        "\n",
        "print(len(input_seqs))\n",
        "print(input_seqs[0].shape)\n",
        "\n",
        "print(len(output_seqs)) \n",
        "print(output_seqs[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "GCD74yATB7_r",
        "outputId": "19afa3b3-c570-4b19-8388-eb6525eb83e1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-bcdc6cc55572>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sent_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sent_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhindi_input_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhindi_input_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0moutput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menglish_output_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menglish_output_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-bcdc6cc55572>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sent_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sent_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhindi_input_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhindi_input_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0moutput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menglish_output_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menglish_output_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'LDAP'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Mydat(Dataset):\n",
        "\n",
        "  def __init__(self, input_seqs, output_seqs):\n",
        "    super().__init__()\n",
        "    self.input_seqs = input_seqs\n",
        "    self.output_seqs = output_seqs\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_seqs)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.input_seqs[index], self.output_seqs[index]"
      ],
      "metadata": {
        "id": "ORYHic9NF8Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Mydat(input_seqs, output_seqs)"
      ],
      "metadata": {
        "id": "aByO6VKqD54t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- making the dataloader"
      ],
      "metadata": {
        "id": "T5I5PTaEGURb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "print(len(train_loader))\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "print(type(batch))\n",
        "print(batch[0].shape) # input_tensors\n",
        "print(batch[1].shape) # output_tensors"
      ],
      "metadata": {
        "id": "9Wh_X9hcGAUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the model"
      ],
      "metadata": {
        "id": "yaVYZBosGs3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D_MODEL = 200\n",
        "NHEAD = 8\n",
        "NUM_ENCODER_LAYERS = 6\n",
        "NUM_DECODER_LAYERS = 6\n",
        "DIM_FEEDFORWARD = 2048\n",
        "DROPOUT = 0.1\n",
        "LR = 0.0005"
      ],
      "metadata": {
        "id": "vJNy-dACGuM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(len(hindi_input_vocab), len(english_output_vocab), D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, DIM_FEEDFORWARD, DROPOUT).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=eng_vocab.word2index['<PAD>'])"
      ],
      "metadata": {
        "id": "YKtGsTFjHDAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "BSyZKnWrHS-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_EPOCHS = 10"
      ],
      "metadata": {
        "id": "U4uquFQxHUOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in tqdm(range(MAX_EPOCHS)):\n",
        "\n",
        "  epoch_loss = 0\n",
        "\n",
        "  for batch in tqdm(train_loader):\n",
        "\n",
        "    input_tensors = batch[0].to(device)\n",
        "    target_tensors = batch[1].to(device)\n",
        "\n",
        "    batch_loss = train_batch(model, optimizer, criterion, input_tensors, target_tensors)\n",
        "\n",
        "    epoch_loss += loss\n",
        "\n",
        "  print(f\"epoch = {epoch}/{max_epochs}, LOSS = {epoch_loss/len(dataloader)}\")\n",
        "\n",
        "  with open('/content/drive/MyDrive/CS779_MT_google_colab/training_loss1.txt', \"a\") as f: \n",
        "        my_dict = {\"epoch\": epoch, \"max_epochs\": max_epochs, \"epoch_loss\": epoch_loss/len(dataloader)}\n",
        "        f.write(f\"{my_dict}\\n\")\n",
        "\n",
        "  torch.save(encoder.state_dict().cpu(), '/content/drive/MyDrive/CS779_MT_google_colab')\n",
        "  torch.save(decoder.state_dict().cpu(), '/content/drive/MyDrive/CS779_MT_google_colab')"
      ],
      "metadata": {
        "id": "IVNtNhruH3XT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}