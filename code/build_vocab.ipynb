{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from indicnlp.normalize import indic_normalize\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 64\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang():\n",
    "\n",
    "  def __init__(self, name, spacy_tokenizer):\n",
    "    self.name = name\n",
    "    self.word2index = {\"<SOS>\":0, '<EOS>': 1, \"<PAD>\": 2, '<UNK>': 3}\n",
    "    self.index2word = {0: \"<SOS>\", 1: \"<EOS>\", 2: \"<PAD>\", 3: '<UNK>'}\n",
    "    self.word2count = {}\n",
    "    self.n_words = 4\n",
    "    self.tokenizer = spacy_tokenizer\n",
    "\n",
    "  def add_word(self, word):\n",
    "    if word not in self.word2index:\n",
    "      self.word2index[word] = self.n_words\n",
    "      self.word2count[word] = 1\n",
    "      self.index2word[self.n_words] = word\n",
    "      self.n_words += 1\n",
    "\n",
    "    else:\n",
    "      self.word2count[word] += 1\n",
    "\n",
    "  def add_sentence(self, sentence):\n",
    "    tokens = self.tokenize_sentence(sentence)\n",
    "    for token in tokens: \n",
    "      self.add_word(token)\n",
    "\n",
    "  def tokenize_sentence(self, sentence):\n",
    "    tokens = [token.text for token in self.tokenizer(sentence.lower())]\n",
    "    return tokens\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hindi_lang():\n",
    "\n",
    "  def __init__(self, name):\n",
    "    self.name = name\n",
    "    self.word2index = {\"<SOS>\":0, '<EOS>': 1, \"<PAD>\": 2, '<UNK>': 3}\n",
    "    self.index2word = {0: \"<SOS>\", 1: \"<EOS>\", 2: \"<PAD>\", 3: '<UNK>'}\n",
    "    self.word2count = {}\n",
    "    self.n_words = 4\n",
    "    self.normalizer = indic_normalize.DevanagariNormalizer(lang='hi', remove_nuktas=True)\n",
    "\n",
    "  def add_word(self, word):\n",
    "    if word not in self.word2index:\n",
    "      self.word2index[word] = self.n_words\n",
    "      self.word2count[word] = 1\n",
    "      self.index2word[self.n_words] = word\n",
    "      self.n_words += 1\n",
    "\n",
    "    else:\n",
    "      self.word2count[word] += 1\n",
    "\n",
    "  def add_sentence(self, sentence):\n",
    "    tokens = self.tokenize_sentence(sentence)\n",
    "    for token in tokens: \n",
    "      self.add_word(token)\n",
    "\n",
    "  def tokenize_sentence(self, sentence):\n",
    "    # first normalize the sentence, then tokenize\n",
    "    norm_sent = self.normalizer.normalize(sentence)\n",
    "    tokens = indic_tokenize.trivial_tokenize(norm_sent)\n",
    "    return tokens\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorFromSentenceEval(lang_vocab, sent, max_length=MAX_LENGTH):\n",
    "    \n",
    "    tokens = lang_vocab.tokenize_sentence(sent)\n",
    "    existing_tokens = lang_vocab.word2index.keys()\n",
    "    \n",
    "    indexes = []\n",
    "    for token in tokens:\n",
    "        if token in existing_tokens:\n",
    "            index = lang_vocab.word2index[token] \n",
    "        else:\n",
    "            index = lang_vocab.word2index['<UNK>']\n",
    "        indexes.append(index)\n",
    "\n",
    "    pad_index = lang_vocab.word2index['<PAD>']\n",
    "\n",
    "    if len(indexes) < max_length:\n",
    "        indexes += [pad_index] * (max_length - len(indexes))\n",
    "\n",
    "    else:\n",
    "        indexes = indexes[:max_length]\n",
    "\n",
    "    return torch.tensor(indexes, dtype=torch.long)\n",
    "    # (max_length,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestSet(Dataset):\n",
    "\n",
    "    def __init__(self, test_sent_tensor_list):\n",
    "        super().__init__()\n",
    "        self.inp = test_sent_tensor_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inp)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.inp[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 1)\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('../datasets/eng_Hindi_data_test_X.csv', header=None)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(लूत की सुनते काहे को) ग़रज़ सूरज निकलते निकलत...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>कि अब तो पकड़े गए मूसा ने कहा हरगिज़ नहीं क्यो...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>खरीदारी सूची बनाएँ (S)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>और जब तुमसे मेरे बन्दे मेरे सम्बन्ध में पूछें,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>और जब वह लौटता है, तो धरती में इसलिए दौड़-धूप ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  (लूत की सुनते काहे को) ग़रज़ सूरज निकलते निकलत...\n",
       "1  कि अब तो पकड़े गए मूसा ने कहा हरगिज़ नहीं क्यो...\n",
       "2                             खरीदारी सूची बनाएँ (S)\n",
       "3  और जब तुमसे मेरे बन्दे मेरे सम्बन्ध में पूछें,...\n",
       "4  और जब वह लौटता है, तो धरती में इसलिए दौड़-धूप ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "test_sent_list = test_df.iloc[:, 0].tolist()\n",
    "print(len(test_sent_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../new/hindi_input_vocab.pkl', 'rb') as f: \n",
    "    hindi_input_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27939\n"
     ]
    }
   ],
   "source": [
    "print(len(hindi_input_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7db6a3260d459e8fcf411be8551425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_tensor_list = []\n",
    "\n",
    "for my_sent in tqdm(test_sent_list):\n",
    "\n",
    "    my_tensor = tensorFromSentenceEval(hindi_input_vocab, my_sent)\n",
    "    test_tensor_list.append(my_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14546,    12,  1169,    53, 12933,   184,  3404,   184,  8728,    15,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor_list[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "test_set = TestSet(test_tensor_list)\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_set, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../new/test_loader.pkl', 'wb') as f: \n",
    "    pickle.dump(test_loader, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, batch, output_vocab):\n",
    "\n",
    "    # batch.shape --> (batch_size, seq_len)\n",
    "\n",
    "    output_tensor = model.forward(batch.to(device), None)\n",
    "    output_tensor = output_tensor.argmax(dim=-1)\n",
    "    # (batch_size, seq_len)\n",
    "\n",
    "    output_list = []\n",
    "    for i in range(output_tensor.size(0)):\n",
    "        output_tokens = [output_tokens.index2word[idx] for idx in output_tensor[i]]\n",
    "        if '<PAD>' in output_tokens:\n",
    "            output_tokens = output_tokens[:output_tokens.index('<PAD>')]\n",
    "        elif '<EOS>' in output_tokens:\n",
    "            output_tokens = output_tokens[:output_tokens.index('<EOS>')]\n",
    "\n",
    "        \n",
    "        output_list.append(' '.join(output_tokens))\n",
    "\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64])\n"
     ]
    }
   ],
   "source": [
    "i = next(iter(test_loader))\n",
    "print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and deliver us by Thy mercy from the people of...</td>\n",
       "      <td>और अपनी रहमत से हमें इन काफ़िर लोगों (के नीचे)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transformed position of fourth point</td>\n",
       "      <td>चौथे बिन्दु का रूपांतरित स्थान</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oh, woe to me; I wish I never took so - and - ...</td>\n",
       "      <td>हाए अफसोस काश मै फला शख्स को अपना दोस्त न बनाता</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The PS file is to be translated into a PDF fil...</td>\n",
       "      <td>पीएस2पीडीएफ के इस्तेमाल से पीएस फ़ाइल को पीडीए...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Receiving LDAP search results...</td>\n",
       "      <td>LDAP खोज परिणाम पा रहा है...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  and deliver us by Thy mercy from the people of...   \n",
       "1               Transformed position of fourth point   \n",
       "2  Oh, woe to me; I wish I never took so - and - ...   \n",
       "3  The PS file is to be translated into a PDF fil...   \n",
       "4                   Receiving LDAP search results...   \n",
       "\n",
       "                                                   1  \n",
       "0  और अपनी रहमत से हमें इन काफ़िर लोगों (के नीचे)...  \n",
       "1                     चौथे बिन्दु का रूपांतरित स्थान  \n",
       "2    हाए अफसोस काश मै फला शख्स को अपना दोस्त न बनाता  \n",
       "3  पीएस2पीडीएफ के इस्तेमाल से पीएस फ़ाइल को पीडीए...  \n",
       "4                       LDAP खोज परिणाम पा रहा है...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('eng_Hindi_data_train.csv',header=None )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and deliver us by Thy mercy from the people of the unbelievers. '\n",
      "140000\n"
     ]
    }
   ],
   "source": [
    "output_sent_all = df.iloc[:, 0].tolist()\n",
    "print(output_sent_all[0])\n",
    "print(len(output_sent_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140000\n",
      "और अपनी रहमत से हमें इन काफ़िर लोगों (के नीचे) से नजात दे\n"
     ]
    }
   ],
   "source": [
    "input_sent_all = df.iloc[:, 1].tolist()\n",
    "print(len(input_sent_all))\n",
    "print(input_sent_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841a75821b884e8aac792ebc36bd6443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27939\n"
     ]
    }
   ],
   "source": [
    "hindi_input_vocab = Hindi_lang(\"hindi\")\n",
    "\n",
    "for my_sent in tqdm(input_sent_all):\n",
    "  hindi_input_vocab.add_sentence(my_sent)\n",
    "\n",
    "print(len(hindi_input_vocab))\n",
    "\n",
    "with open('new/hindi_input_vocab.pkl', 'wb') as f: \n",
    "  pickle.dump(hindi_input_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f1f18af9f5473f90b447ce5e079a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24261\n"
     ]
    }
   ],
   "source": [
    "nlp_english = spacy.load(\"en_core_web_sm\")\n",
    "english_output_vocab = Lang(\"english\", nlp_english)\n",
    "\n",
    "for my_sent in tqdm(output_sent_all):\n",
    "  english_output_vocab.add_sentence(my_sent)\n",
    "\n",
    "print(len(english_output_vocab))\n",
    "\n",
    "# saving the english vocab\n",
    "\n",
    "with open('new/english_output_vocab.pkl', 'wb') as f: \n",
    "  pickle.dump(english_output_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new/hindi_input_vocab.pkl', 'rb') as f: \n",
    "    hindi_input_vocab = pickle.load(f)\n",
    "\n",
    "with open('new/english_output_vocab.pkl', 'rb') as f: \n",
    "    english_output_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out_sent.pkl', 'rb') as f: \n",
    "    input_sent_all = pickle.load(f)\n",
    "\n",
    "with open('inp_sent.pkl', 'rb') as f: \n",
    "    output_sent_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS = 1, PAD = 2, SOS = 0\n"
     ]
    }
   ],
   "source": [
    "EOS_TOKEN_INDEX = hindi_input_vocab.word2index['<EOS>']\n",
    "PAD_TOKEN_INDEX = hindi_input_vocab.word2index['<PAD>']\n",
    "SOS_TOKEN_INDEX = hindi_input_vocab.word2index['<SOS>']\n",
    "\n",
    "print(f\"EOS = {EOS_TOKEN_INDEX}, PAD = {PAD_TOKEN_INDEX}, SOS = {SOS_TOKEN_INDEX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2d2b2e106847e492a305f08945ab1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140000\n",
      "torch.Size([64])\n",
      "140000\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "input_seqs = []\n",
    "output_seqs = []\n",
    "\n",
    "for input_sent, output_sent in tqdm(zip(input_sent_all, output_sent_all)):\n",
    "  input_seq = [hindi_input_vocab.word2index[word] for word in hindi_input_vocab.tokenize_sentence(input_sent)]\n",
    "  output_seq = [english_output_vocab.word2index[word] for word in english_output_vocab.tokenize_sentence(output_sent)]\n",
    "\n",
    "  output_seq = [SOS_TOKEN_INDEX] + output_seq + [EOS_TOKEN_INDEX]\n",
    "\n",
    "  if len(input_seq) < MAX_LENGTH:\n",
    "    input_seq += [hindi_input_vocab.word2index['<PAD>']] * (MAX_LENGTH - len(input_seq))\n",
    "  else:\n",
    "    input_seq = input_seq[:64]\n",
    "\n",
    "  if len(output_seq) < MAX_LENGTH:\n",
    "    output_seq += [english_output_vocab.word2index['<PAD>']] * (MAX_LENGTH - len(output_seq))\n",
    "  else:\n",
    "    output_seq = output_seq[:MAX_LENGTH]\n",
    "    output_seq[-1] = EOS_TOKEN_INDEX\n",
    "\n",
    "  input_seqs.append(torch.tensor(input_seq, dtype=torch.long))\n",
    "  output_seqs.append(torch.tensor(output_seq, dtype=torch.long))\n",
    "\n",
    "print(len(input_seqs))\n",
    "print(input_seqs[0].shape)\n",
    "\n",
    "print(len(output_seqs))\n",
    "print(output_seqs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 11, 14, 15, 16,  1,  2,  2,\n",
       "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mydat(Dataset):\n",
    "\n",
    "  def __init__(self, input_seqs, output_seqs):\n",
    "    super().__init__()\n",
    "    self.input_seqs = input_seqs\n",
    "    self.output_seqs = output_seqs\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_seqs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.input_seqs[index], self.output_seqs[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140000\n"
     ]
    }
   ],
   "source": [
    "my_dataset = Mydat(input_seqs, output_seqs)\n",
    "print(len(my_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new/dataset.pkl', 'wb') as f: \n",
    "    pickle.dump(my_dataset, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(32, 64)\n",
    "b = a[0:1, :]\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = Mydat(test_sent_list, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_loader = DataLoader(, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3, 4, 5)\n",
    "b = a.argmax(dim=-1)\n",
    "\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
